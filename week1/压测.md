### 环境说明

目前仅有3台天翼云服务器，均为4C32G。配置了kubernetes1.26集群，分为一主两从，其中网络为固定ip固定带宽4MBps。

以下均以此为基础，后续可能按情况添加临时服务器。



### 搭建压测监控平台

##### 1.prometheus & grafana 

已经通过kube-prometheus-stack在集群部署好了，本地部署新的可能会有冲突，所以直接拿过来用了。

部署github链接：https://github.com/prometheus-community/helm-charts



##### 2.jmeter

###### 部署slaves

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jmeter-slaves
  namespace: jmeter-exercise
  labels:
    jmeter_mode: slave
spec:
  replicas: 2
  selector:
    matchLabels:
      jmeter_mode: slave
  template:
    metadata:
      labels:
        jmeter_mode: slave
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: jmeter_mode
                  operator: In
                  values:
                  - slave
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: jmslave
        image: justb4/jmeter:5.5
        imagePullPolicy: IfNotPresent
        env:
          - name: TZ
            value: Asia/Shanghai       
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh","-c","mkdir -p /jmeter/jmx"]
        command: 
          - "jmeter-server"
          - "-Dserver.rmi.localport=50000"
          - "-Dserver_port=1099"
          - "-Jserver.rmi.ssl.disable=true"
        ports:
        - containerPort: 1099
        - containerPort: 50000
---
apiVersion: v1
kind: Service
metadata:
  name: jmeter-slaves-svc
  namespace: jmeter-exercise
  labels:
    jmeter_mode: slave
spec:
  clusterIP: None
  ports:
    - port: 1099
      name: first
      targetPort: 1099
    - port: 50000
      name: second
      targetPort: 50000
  selector:
    jmeter_mode: slave
```

###### 部署master

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: jmeter-run
  namespace: jmeter-exercise
  labels:
    app: jmeter-run
data:
  run.sh: |
    #!/bin/bash
    rm -rf /jemter/jtl/test.jtl /jmeter/web/*
    jmeter -Dserver.rmi.ssl.disable=true -n -t /jmeter/jmx/test.jmx -l /jemter/jtl/test.jtl -e -o /jmeter/web/ \
      -R `getent ahostsv4 jmeter-slaves-svc | cut -d' ' -f1 | sort -u | awk -v ORS=, '{print $1}' | sed 's/,$//'`
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jmeter-master
  namespace: jmeter-exercise
  labels:
    app: jmeter-master
    jmeter_mode: master
spec:
  replicas: 1
  selector:
    matchLabels:
      jmeter_mode: master
  template:
    metadata:
      labels:
        jmeter_mode: master
    spec:
      containers:
      - name: jmmaster
        image: justb4/jmeter:5.5
        imagePullPolicy: IfNotPresent
        env:
          - name: TZ
            value: Asia/Shanghai       
        lifecycle:
          postStart:
            exec:
              command: ["/bin/bash","-c","mkdir -p /jmeter/{jmx,jtl,web}"]
        command: [ "/bin/bash", "-c", "--" ]
        args: [ "while true; do sleep 30; done;" ]
        ports:
        - containerPort: 60000
        volumeMounts:
          - name: jmeter-cm
            mountPath: /run.sh
            subPath: run.sh
          - name: shared-data
            mountPath: /jmeter/web
      - name: nginx
        image: nginx:stable-alpine
        readinessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 5      # 延迟加载时间
          periodSeconds: 10             # 重试时间间隔
          failureThreshold: 6          # 不健康阈值       
        volumeMounts:
          - name: shared-data
            mountPath: /usr/share/nginx/html
      volumes:
      - name: jmeter-cm
        configMap:
         name: jmeter-run
      - name: shared-data
        emptyDir: {}
      tolerations:
        - key: node-role.kubernetes.io/control-plane # 设置master的容忍度
          effect: NoSchedule
          operator: Exists
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: NotIn
                values:
                  - worker-1
                  - worker-2
---
apiVersion: v1
kind: Service
metadata:
  name: jmeter-nginx
  labels:
    app: jmeter-nginx
  namespace: jmeter-exercise
spec:
  type: NodePort
  ports:
  - name: jmeterport
    port: 80
    targetPort: 80
    nodePort: 30144
  selector:
    jmeter_mode: master 
```



###### 说明

由于是容器部署，所以对于执行文件需要单独拷贝进容器，并且安装了插件的情况下需要把插件管理器的jar也同样拷贝至容器的 `/opt/apache-jmeter/lib/ext` 的目录下。

PS：master设置了nginx 可以直接访问 `IP:30144`查看结果

例：

![MASTER-网页nginx显示](.\picture\MASTER-网页nginx显示.png)



###### 编写sh脚本

```
#!/bin/bash

jmx="03-jmeter-example1.1k.jmx"  # jmx压测文件
#csv="data_list.csv" # csv文件，在jmx中路径为/jmeter/jmx/data_list；若无，同时注释掉Step2

echo "Step1 拷贝jmx至master"
master_pod=`kubectl get po -n jmeter-exercise | grep jmeter-master | grep Running | awk '{print $1}'`
kubectl cp -n jmeter-exercise "$jmx" -c jmmaster  "$master_pod:/jmeter/jmx/test.jmx"

echo "Step2 拷贝csv至slave"
#slave_pod=`kubectl get po -n jmeter-exercise | grep jmeter-slave | grep Running | awk '{print $1}'`
#for i in $slave_pod;do
#  kubectl cp "$csv"  "$i:/jmeter/jmx/$csv"
#done;

echo "Step3 开启压测"
kubectl exec -ti -n jmeter-exercise $master_pod -c jmmaster -- bash /run.sh

echo "Step4 查询压测结果："
echo "http://ip:30144"
```



##### 3.influxdb

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: influxdb
  namespace: jmeter-exercise
  labels:
    app: influxdb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: influxdb
  template:
    metadata:
      name: influxdb
      labels:
        app: influxdb
    spec:
      containers:
        - name: influxdb
          image: influxdb:1.8.10
          volumeMounts:
            - mountPath: /data
              name: influxdb-storage
      volumes:
        - name: influxdb-storage
          emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: influxdb
  namespace: jmeter-exercise
spec:
  type: NodePort
  selector:
    app: influxdb
  ports:
    - name: influxp
      port: 8086
      targetPort: 8086
      nodePort: 32105
```



进入容器创建新库

```shell
# 进入容器
influx
# 查看数据库
show databases
# 创建 jmeterdb
CREATE DATABASE jmeterdb
```

环境基础部署完成，再根据视频添加 grafana 对应的influx后台就ok了。



##### 4.jmx脚本

由于master也是基于Linux服务器，所以目前在本地win服务器上修改《资料2》下的jmx脚本，修改参数后上传到对应服务器即可。









> 说明：

基于《项目性能测试报告》作为模板,使用《资料》里的jar与测试案例，修改内容进行测试。



# 项目性能测试报告

## 01-测试目的

主要是让开发者对hero_mall项目的性能负载和容量有个准确的认知。同时，协助技术管理者更好的管理业务系统性能质量，科学评估业务系统的负荷，拒绝盲目上线。

## 02-测试工具

![image-20220806185532353](.\picture\图.jpg)

## 03-测试环境

### 3.1 环境

| 指标              | 参数    |
| ----------------- | ------- |
| 机器              | 4C32G   |
| 集群规模          | 3集群   |
| hero_mall_one版本 | 1.0     |
| 数据库版本        | mysql 8 |

### 3.1 设置启动参数

由于时间关系，暂时还不会把镜像打入k8s集群，目前仍以jar方式启动（目前为部署在单节点的服务）

```bash
export JAVA_HOME
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib


export SERVER_NAME="hero_web"
export JAVA="$JAVA_HOME/bin/java"
export BASE_DIR=`cd $(dirname $0)/.; pwd`
export DEFAULT_SEARCH_LOCATIONS="classpath:/,classpath:/config/,file:./,file:./config/"
export CUSTOM_SEARCH_LOCATIONS=${DEFAULT_SEARCH_LOCATIONS},file:${BASE_DIR}/conf/

JAVA_OPT="${JAVA_OPT} -server -Xms512m -Xmx512m -Xmn256 -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"
JAVA_OPT="${JAVA_OPT} -XX:-OmitStackTraceInFastThrow -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${BASE_DIR}/logs/java_heapdump.hprof"
JAVA_OPT="${JAVA_OPT} -XX:-UseLargePages"
JAVA_OPT="${JAVA_OPT} -jar ${BASE_DIR}/${SERVER_NAME}*.jar"
JAVA_OPT="${JAVA_OPT} ${JAVA_OPT_EXT}"

JAVA_OPT="${JAVA_OPT} --spring.config.location=${CUSTOM_SEARCH_LOCATIONS}"
if [ ! -d "${BASE_DIR}/logs" ]; then
  mkdir ${BASE_DIR}/logs
fi
echo "$JAVA ${JAVA_OPT}"

if [ ! -f "${BASE_DIR}/logs/${SERVER_NAME}.out" ]; then
  touch "${BASE_DIR}/logs/${SERVER_NAME}.out"
fi

echo "$JAVA ${JAVA_OPT}" > ${BASE_DIR}/logs/${SERVER_NAME}.out 2>&1 &
nohup $JAVA ${JAVA_OPT} hero_web.hero_web >> ${BASE_DIR}/logs/${SERVER_NAME}.out 2>&1 &
echo "server is starting，you can check the ${BASE_DIR}/logs/${SERVER_NAME}.out"
```



## 04-测试场景

测试场景一般情况下是都是最重要接口：验证hero_mall服务获取商品信息接口在不同并发规模的表现

**情况01-模拟低延时场景，**用户访问接口并发逐渐增加的过程。接口的响应时间为20ms，线程梯度：5、10、15、20、25、30、35、40个线程，5000次;

- 时间设置：Ramp-up period(inseconds)的值设为对应线程数
- 测试总时长：约等于20ms x 5000次 x 8 = 800s = 13分

**情况02-模拟高延时场景，**用户访问接口并发逐渐增加的过程。接口的响应时间为500ms，线程梯度：100、200、300、400、500、600、700、800个线程，200次; 

- 时间设置：Ramp-up period(inseconds)的值设为对应线程数的1/10；
- 测试总时长：约等于500ms x 200次 x 8 = 800s = 13分



目前仅测试普通场景，未测试包含OpenResty 与 redis缓存优化场景。



## 05-核心接口的测试结果

### 一、接口测试

#### 1、低延迟20ms-响应1.1k

部署了 03-jmeter-example-低延迟20ms-响应1.1k.jmx

测试脚本为低延时脚本，但根据grafana 可以看出网络是最大瓶颈 其余资源完全充裕，可以通过提升带宽来优化。

![网络瓶颈](C:\Users\AdminZz\Desktop\homework\picture\网络瓶颈.png)



#### 2、高延迟500ms-800线程

部署了 04-jmeter-example-高延迟500ms-800线程.jmx

![800线程-2](C:\Users\AdminZz\Desktop\homework\picture\800线程-2.png)



由图可以看到 当跑高延迟 多线程并发情况下，带宽依旧是问题，不过此时还新增了新的问题，异常率已经超过了50%以上。此时需要提升服务性能需要进行多种优化方案才行。





## 06-测试结论

hero_web性能测试是针对重点功能，单机单节点服务进行压测，可以看到各个接口容量。目前所测试结果最大瓶颈为网络带宽。



## 07-补充说明

通过压测结果可以看出各项指标服务的瓶颈，包含的有CPU、内存、IO、Network等等。

对应相应的优化也有：Tomcat调优、IO模型调优、Web容器、缓存、数据库、JVM等等。

对于并发访问提高吞吐量可进行提升Tomcat的线程数；对于系统io可使用更先进的io模型，尽量不要使用同步非阻塞；web容器可更换性能更强劲的项目；对于DB可进行更换DB服务，或者进行DB的分表分库；使用缓存优化方案，越接近用户端的缓存越有效果。



